# -*- coding: utf-8 -*-
"""NLU - Lab8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oz-cUnnn0ehBF8PIX-bh_gJxya-OAey-

# Pre-Exercise

## 2. WordNet
"""

import nltk
from pprint import pprint
nltk.download('wordnet')
nltk.download('omw-1.4')

# Let's import WordNet
from nltk.corpus import wordnet

# printing senses of a word (including honomymy & polysemy)
senses = wordnet.synsets('bank')
print(senses)

"""## 2.1. Synset"""

# it's possible to provide part of speech to filter senses as well
senses = wordnet.synsets('bank', wordnet.NOUN)
pprint(senses)
print('')
print("POS:",senses[0].pos())  # part-of-speech tag of a synset

print(senses[0].definition())
print(senses[0].examples())

"""## 2.2. Lemmatization"""

wordnet.morphy('banked')

# Note that only verb synsets are listed
wordnet.synsets('banked')

from nltk.stem import WordNetLemmatizer
lem = WordNetLemmatizer()
print(lem.lemmatize('banks'))
print(lem.lemmatize('banked', pos=wordnet.VERB))
print(lem.lemmatize('bnked', pos=wordnet.VERB))  # returns the word itself if it cannot find it

"""## 2.2.1. Lemmas in WordNet"""

lemmas = wordnet.lemmas('bank')
pprint(lemmas)

# Look up lemma directly
lemma = wordnet.lemma('bank.n.01.bank')
print(lemma.name())
print(lemma.synset())

# Get Lemmas of a synset
print(senses[0].lemmas())
print(senses[0].lemma_names())

"""## 2.3. Lexical Relations beween Synsets"""

pprint(senses[0].hyponyms())
pprint(senses[0].hypernyms())

# getting paths to the root of the taxonomy
pprint(senses[0].hypernym_paths())
# getting hypernyms with distances
pprint(senses[0].hypernym_distances())
# getting the root node
pprint(senses[0].root_hypernyms())
print(senses[0].max_depth())
print(senses[0].min_depth())

"""## 3.4. Using Lesk in NLTK"""

from nltk.wsd import lesk

sense = lesk('Jane sat on the sloping bank of a river beside the water'.split(), 'bank')
print(sense)
print(sense.definition())

# possible to specify the POS
print(lesk('Jane sat on the sloping bank of a river beside the water'.split(),
           'bank',
           pos=wordnet.NOUN))

# possible to specify the synsets to choose from
print(lesk('Jane sat on the sloping bank of a river beside the water'.split(),
           'bank',
           synsets=wordnet.synsets('riverbank')))

"""## Exercise
Even though NLTK states that it implements Original Lesk Algorithm, in fact it is a Simplified Lesk Algorithm, that doesn't consider examples, and computes overlaps like the original.

In the original algorithm context is computed differently. Instead of comparing a target word's signature with the context words, the target signature is compared with the signatures of each of the context words.

Implement the Original Lesk Algorithm (modifying NLTK's, see pseudocode above) Todo list:

* Complete lesk simplified

* Preprocessing:

  * compute pos-tag with nltk.pos_tag
  * remove stopwords
    * from nltk.corpus import stopwords
    * stopwords.words('english')
* take the majority decision (the sense predicted most frequently)
"""

!pip install stopwords

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('punkt')

def preprocess(text):
    mapping = {"NOUN": wordnet.NOUN, "VERB": wordnet.VERB, "ADJ": wordnet.ADJ, "ADV": wordnet.ADV}
    sw_list = stopwords.words('english')

    lem = WordNetLemmatizer()

    # tokenize, if input is text
    tokens = nltk.word_tokenize(text) if type(text) is str else text
    # pos-tag
    tagged = nltk.pos_tag(tokens, tagset="universal")
    # lowercase
    tagged = [(w.lower(), p) for w, p in tagged]
    # optional: remove all words that are not NOUN, VERB, ADJ, or ADV (i.e. no sense in WordNet)
    tagged = [(w, p) for w, p in tagged if p in mapping]
    # re-map tags to WordNet (return orignal if not in-mapping, if above is not used)
    tagged = [(w, mapping.get(p, p)) for w, p in tagged]
    # remove stopwords
    tagged = [(w, p) for w, p in tagged if w not in sw_list]
    # lemmatize
    tagged = [(w, lem.lemmatize(w, pos=p), p) for w, p in tagged]
    # unique the list
    tagged = list(set(tagged))
    return tagged

def get_sense_definitions(context):
    # input is text or list of strings
    lemma_tags = preprocess(context)
    # let's get senses for each
    senses = [(w, wordnet.synsets(l, p)) for w, l, p in lemma_tags]

    # let's get their definitions
    definitions = []
    for raw_word, sense_list in senses:
        if len(sense_list) > 0:
            # let's tokenize, lowercase & remove stop words
            def_list = []
            for s in sense_list:
                defn = s.definition()
                # let's use the same preprocessing
                tags = preprocess(defn)
                toks = [l for w, l, p in tags]
                def_list.append((s, toks))
            definitions.append((raw_word, def_list))
    return definitions

def get_top_sense(words, sense_list):
    # get top sense from the list of sense-definition tuples
    # assumes that words and definitions are preprocessed identically
    val, sense = max((len(set(words).intersection(set(defn))), ss) for ss, defn in sense_list)
    return val, sense

from collections import Counter
def lesk_simplified(context_sentence, ambiguous_word, pos=None, synsets=None):
    context = set(context_sentence)

    if synsets is None:
        synsets = wordnet.synsets(ambiguous_word)
    if pos:
        synsets = [ss for ss in synsets if str(ss.pos()) == pos]

    if not synsets:
        return None
    # Measure the overlap between context and definitions
    _, sense = max(
        (len(context.intersection(ss.definition().split())), ss) for ss in synsets
    )

    return sense

def original_lesk(context_sentence, ambiguous_word, pos=None, synsets=None, majority=False):

    context_senses = get_sense_definitions(set(context_sentence)-set([ambiguous_word]))
    if synsets is None:
        synsets = get_sense_definitions(ambiguous_word)[0][1]

    if pos:
        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]

    if not synsets:
        return None
    scores = []
    # print(synsets)
    for senses in context_senses:
        for sense in senses[1]:
            scores.append(get_top_sense(sense[1], synsets))

    if len(scores) == 0:
        return synsets[0][0]

    if majority:
        filtered_scores = [x[1] for x in scores if x[0] != 0]
        if len(filtered_scores) > 0:
            best_sense = Counter(filtered_scores).most_common(1)[0][0]
        else:
            # Almost random selection
            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]
    else:
        _, best_sense = max(scores)
    return best_sense

text = "Jane sat on the sloping bank of a river beside the water".split()
word = "bank"
print("Sense from lesk original", original_lesk(text, word, majority=True))
print("Sense from lesk simplified", lesk_simplified(text, word))
print("Sense from lesk NLTK", lesk(text, word))

"""## 4.1.2. Path-based Similarity"""

bank_r = wordnet.synsets('bank')[0]
bank_f = wordnet.synsets('bank')[1]
river = wordnet.synsets('river')[0]
school = wordnet.synsets('school')[0]

print(river.definition())
print(bank_r.definition())
print(bank_f.definition())
print(school.definition())

print(bank_r.path_similarity(river))
print(bank_f.path_similarity(river))

print(bank_f.path_similarity(school))

print(bank_r.lch_similarity(river))
print(bank_f.lch_similarity(river))
print(bank_f.lch_similarity(school))

print(bank_r.wup_similarity(river))
print(bank_f.wup_similarity(river))
print(bank_f.wup_similarity(school))

"""## 4.1.3. Information Content Similarity"""

# getting pre-computed ic of the semcor corpus (large sense tagged corpus)
from nltk.corpus import wordnet_ic
nltk.download('wordnet_ic')
semcor_ic = wordnet_ic.ic('ic-semcor.dat')

print(bank_r.res_similarity(river, semcor_ic))
print(bank_f.res_similarity(river, semcor_ic))
print(bank_f.res_similarity(school, semcor_ic))

print(bank_r.lin_similarity(bank_r, semcor_ic))
print(bank_f.lin_similarity(river, semcor_ic))
print(bank_f.lin_similarity(school, semcor_ic))

print(bank_r.jcn_similarity(bank_r, semcor_ic))
print(bank_f.jcn_similarity(river, semcor_ic))
print(bank_f.jcn_similarity(school, semcor_ic))

"""## Exercise
Extend Lesk algorithm (function) to use similarity metrics instead of just overlaps

* add a keyword argument to allow different metrics

Complete the Predersen algorithm with the similiratiy metrics

* compare the output of the two algorithms
"""

semcor_ic = wordnet_ic.ic('ic-semcor.dat')
def get_top_sense_sim(context_sense, sense_list, similarity):
    # get top sense from the list of sense-definition tuples
    # assumes that words and definitions are preprocessed identically
    scores = []
    for sense in sense_list:
        ss = sense[0]
        if similarity == "path":
            try:
                scores.append((context_sense.path_similarity(ss), ss))
            except:
                scores.append((0, ss))
        elif similarity == "lch":
            try:
                scores.append((context_sense.lch_similarity(ss), ss))
            except:
                scores.append((0, ss))
        elif similarity == "wup":
            try:
                scores.append((context_sense.wup_similarity(ss), ss))
            except:
                scores.append((0, ss))
        elif similarity == "resnik":
            try:
                scores.append((context_sense.res_similarity(ss, semcor_ic), ss))
            except:
                scores.append((0, ss))
        elif similarity == "lin":
            try:
                scores.append((context_sense.lin_similarity(ss, semcor_ic), ss))
            except:
                scores.append((0, ss))
        elif similarity == "jiang":
            try:
                scores.append((context_sense.jcn_similarity(ss, semcor_ic), ss))
            except:
                scores.append((0, ss))
        else:
            print("Similarity metric not found")
            return None
    val, sense = max(scores)
    return val, sense

def lesk_similarity(context_sentence, ambiguous_word, similarity="resnik", pos=None,
                    synsets=None, majority=True):
    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))

    if synsets is None:
        synsets = get_sense_definitions(ambiguous_word)[0][1]

    if pos:
        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]

    if not synsets:
        return None

    scores = []

    # Here you may have some room for improvement
    # For instance instead of using all the definitions from the context
    # you pick the most common one of each word (i.e. the first)
    for senses in context_senses:
        for sense in senses[1]:
            scores.append(get_top_sense_sim(sense[0], synsets, similarity))

    if len(scores) == 0:
        return synsets[0][0]

    if majority:
        filtered_scores = [x[1] for x in scores if x[0] != 0]
        if len(filtered_scores) > 0:
            best_sense = Counter(filtered_scores).most_common(1)[0][0]
        else:
            # Almost random selection
            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]
    else:
        _, best_sense = max(scores)

    return best_sense

def predersen(context_sentence, ambiguous_word, similarity="resnik", pos=None,
                    synsets=None, threshold=0.1):

    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))

    if synsets is None:
        synsets = get_sense_definitions(ambiguous_word)[0][1]

    if pos:
        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]

    if not synsets:
        return None

    synsets_scores = {}
    for ss_tup in synsets:
        ss = ss_tup[0]
        if ss not in synsets_scores:
            synsets_scores[ss] = 0
        for senses in context_senses:
            scores = []
            for sense in senses[1]:
                if similarity == "path":
                    try:
                        scores.append((sense[0].path_similarity(ss), ss))
                    except:
                        scores.append((0, ss))
                elif similarity == "lch":
                    try:
                        scores.append((sense[0].lch_similarity(ss), ss))
                    except:
                        scores.append((0, ss))
                elif similarity == "wup":
                    try:
                        scores.append((sense[0].wup_similarity(ss), ss))
                    except:
                        scores.append((0, ss))
                elif similarity == "resnik":
                    try:
                        scores.append((sense[0].res_similarity(ss, semcor_ic), ss))
                    except:
                        scores.append((0, ss))
                elif similarity == "lin":
                    try:
                        scores.append((sense[0].lin_similarity(ss, semcor_ic), ss))
                    except:
                        scores.append((0, ss))
                elif similarity == "jiang":
                    try:
                        scores.append((sense[0].jcn_similarity(ss, semcor_ic), ss))
                    except:
                        scores.append((0, ss))
                else:
                    print("Similarity metric not found")
                    return None
            value, sense = max(scores)
            if value > threshold:
                synsets_scores[sense] = synsets_scores[sense] + value

    values = list(synsets_scores.values())
    if sum(values) == 0:
        print('Warning all the scores are 0')
    senses = list(synsets_scores.keys())
    best_sense_id = values.index(max(values))
    return senses[best_sense_id]

text = "Jane sat on the sloping bank of a river beside the water".split()
word = "bank"
sense = original_lesk(text, word, majority=True)
print('Original lesk', sense, sense.definition())
sense = lesk(text, word)
print('Symplified lesk', sense, sense.definition())
sense = predersen(text, word, similarity="resnik", threshold=0.1)
print("Pedersen", sense, sense.definition())
sense = lesk_similarity(text, word, pos='n', similarity="path")
print('Graph-based lesk', sense, sense.definition())

"""## 5. Evaluation on Senseval 2"""

nltk.download('senseval')

from nltk.corpus import senseval

inst = senseval.instances('interest.pos')[0]

print(inst.position, inst.context, inst.senses)

"""## 5.1.1. Mapping Senseval Senses to WordNet"""

# definitions of "interest"'s synsets in WordNet
iss = wordnet.synsets('interest', pos='n')
for ss in iss:
    print(ss, ss.definition())

# Let's create mapping from convenience
mapping = {
    'interest_1': 'interest.n.01',
    'interest_2': 'interest.n.03',
    'interest_3': 'pastime.n.01',
    'interest_4': 'sake.n.01',
    'interest_5': 'interest.n.05',
    'interest_6': 'interest.n.04',
}

"""## 5.1.2. Evaluation"""

from nltk.metrics.scores import precision, recall, f_measure, accuracy

refs = {k: set() for k in mapping.values()}
hyps = {k: set() for k in mapping.values()}
refs_list = []
hyps_list = []

# since WordNet defines more senses, let's restrict predictions
synsets = [ss for ss in wordnet.synsets('interest', pos='n') if ss.name() in mapping.values()]

for i, inst in enumerate(senseval.instances('interest.pos')):
    txt = [t[0] for t in inst.context]
    raw_ref = inst.senses[0] # let's get first sense
    hyp = lesk_simplified(txt, txt[inst.position], synsets=synsets).name()
    ref = mapping.get(raw_ref)

    # for precision, recall, f-measure
    refs[ref].add(i)
    hyps[hyp].add(i)

    # for accuracy
    refs_list.append(ref)
    hyps_list.append(hyp)

print("Acc:", round(accuracy(refs_list, hyps_list), 3))

for cls in hyps.keys():
    p = precision(refs[cls], hyps[cls])
    r = recall(refs[cls], hyps[cls])
    f = f_measure(refs[cls], hyps[cls], alpha=1)

    print("{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}".format(cls, p, r, f, len(refs[cls])))

"""## Exercise
* Evaluate Original Lesk (your implementation on Senseval's interest)
* You can also easily evalutate Lesk similarity that we have seen before
"""

from nltk.metrics.scores import precision, recall, f_measure, accuracy

refs = {k: set() for k in mapping.values()}
hyps = {k: set() for k in mapping.values()}
refs_list = []
hyps_list = []

# since WordNet defines more senses, let's restrict predictions

synsets = []
for ss in wordnet.synsets('interest', pos='n'):
    if ss.name() in mapping.values():
        defn = ss.definition()
        tags = preprocess(defn)
        toks = [l for w, l, p in tags]
        synsets.append((ss,toks))

for i, inst in enumerate(senseval.instances('interest.pos')):
    txt = [t[0] for t in inst.context]
    raw_ref = inst.senses[0] # let's get first sense
    hyp = original_lesk(txt, txt[inst.position], synsets=synsets, majority=True).name()
    ref = mapping.get(raw_ref)

    # for precision, recall, f-measure
    refs[ref].add(i)
    hyps[hyp].add(i)

    # for accuracy
    refs_list.append(ref)
    hyps_list.append(hyp)

print("Acc:", round(accuracy(refs_list, hyps_list), 3))

for cls in hyps.keys():
    p = precision(refs[cls], hyps[cls])
    r = recall(refs[cls], hyps[cls])
    f = f_measure(refs[cls], hyps[cls], alpha=1)

    print("{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}".format(cls, p, r, f, len(refs[cls])))

"""## 6.1.1. Bag-of-Words (BOW) Classification (recap)"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_validate
from sklearn.metrics import classification_report
from sklearn.model_selection import StratifiedKFold

data = [" ".join([t[0] for t in inst.context]) for inst in senseval.instances('interest.pos')]
lbls = [inst.senses[0] for inst in senseval.instances('interest.pos')]

print(data[0])
print(lbls[0])

vectorizer = CountVectorizer()
classifier = MultinomialNB()
lblencoder = LabelEncoder()

stratified_split = StratifiedKFold(n_splits=5, shuffle=True)

vectors = vectorizer.fit_transform(data)

# encoding labels for multi-calss
lblencoder.fit(lbls)
labels = lblencoder.transform(lbls)

scores = cross_validate(classifier, vectors, labels, cv=stratified_split, scoring=['f1_micro'])

print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))

"""## Collocational Features"""

def collocational_features(inst):
    p = inst.position
    return {
        "w-2_word": 'NULL' if p < 2 else inst.context[p-2][0],
        "w-1_word": 'NULL' if p < 1 else inst.context[p-1][0],
        "w+1_word": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][0],
        "w+2_word": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][0]
    }

data_col = [collocational_features(inst) for inst in senseval.instances('interest.pos')]
print(data_col[0])

from sklearn.feature_extraction import DictVectorizer
dvectorizer = DictVectorizer(sparse=False)
dvectors = dvectorizer.fit_transform(data_col)

scores = cross_validate(classifier, dvectors, labels, cv=stratified_split, scoring=['f1_micro'])

print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))

"""## 6.1.3. Concatenating Feature Vectors"""

import numpy as np

# let's check shape's for sanity & types (for illustration)
print(vectors.shape, type(vectors))
print(dvectors.shape, type(dvectors))

# types of CountVectorizer and DictVectorizer outputs are different
# we need to convert them to the same format
uvectors = np.concatenate((vectors.toarray(), dvectors), axis=1)

print(uvectors.shape, type(uvectors))

# cross-validating classifier the usual way
scores = cross_validate(classifier, uvectors, labels, cv=stratified_split, scoring=['f1_micro'])

print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))

"""# Lab Exercise
**Same test set for all the experiments, you can use K-fold validation**

* Extend collocational features with
  * POS-tags
  * Ngrams within window
* Concatenate BOW and new collocational feature vectors & evaluate
* Evaluate Lesk Original and Graph-based (Lesk Similarity or Pedersen) metrics on the same test split and compare

## Import & Installing
"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('senseval')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('universal_tagset')
from nltk.corpus import senseval
inst = senseval.instances('interest.pos')[0]
from sklearn.feature_extraction import DictVectorizer
from sklearn.model_selection import cross_validate
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from nltk import ngrams
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
from nltk.metrics.scores import precision, recall, f_measure, accuracy
from collections import Counter
from tqdm import tqdm

"""## Extend Collocational Features with POS-Tags"""

def collocational_features(inst):
    p = inst.position
    return {
        "w-2_word": 'NULL' if p < 2 else inst.context[p-2][0],
        "w-2_pos": 'NULL' if p < 2 else inst.context[p-2][1],
        "w-1_word": 'NULL' if p < 1 else inst.context[p-1][0],
        "w-1_pos": 'NULL' if p < 1 else inst.context[p-1][1],
        "w+1_word": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][0],
        "w+1_pos": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][1],
        "w+2_word": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][0],
        "w+2_pos": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][1]
    }

data_col = [collocational_features(inst) for inst in senseval.instances('interest.pos')]
print(data_col[0])
lbls = [inst.senses[0] for inst in senseval.instances('interest.pos')]

dvectorizer = DictVectorizer(sparse=False)
dvectors = dvectorizer.fit_transform(data_col)
classifier = MultinomialNB()
lblencoder = LabelEncoder()

lblencoder.fit(lbls)
labels = lblencoder.transform(lbls)

stratified_split = StratifiedKFold(n_splits=5, shuffle=True)

scores = cross_validate(classifier, dvectors, labels, cv=stratified_split, scoring=['f1_micro'])

print("Extend with POS-Tags:")
print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))

"""## Extend Collocational Features with Ngrams within window"""

def collocational_features(inst, n=2):
    context_words = [token[0] for token in inst.context]
    context_pos = [token[1] for token in inst.context]
    ngrams_within_window = list(ngrams(context_words, n))
    ngram_features = {}
    p = inst.position
    for i, ngram in enumerate(ngrams_within_window):
        feature_name = f"ngram_{i}"
        ngram_features[feature_name] = " ".join(ngram)
    features = {
        "w-2_word": 'NULL' if p < 2 else inst.context[p-2][0],
        "w-2_pos": 'NULL' if p < 2 else inst.context[p-2][1],
        "w-1_word": 'NULL' if p < 1 else inst.context[p-1][0],
        "w-1_pos": 'NULL' if p < 1 else inst.context[p-1][1],
        "w+1_word": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][0],
        "w+1_pos": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][1],
        "w+2_word": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][0],
        "w+2_pos": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][1]
    }
    features.update(ngram_features)
    return features

data_col = [collocational_features(inst) for inst in senseval.instances('interest.pos')]
print(data_col[0])
lbls = [inst.senses[0] for inst in senseval.instances('interest.pos')]

dvectorizer = DictVectorizer(sparse=False)
dvectors = dvectorizer.fit_transform(data_col)
classifier = MultinomialNB()
lblencoder = LabelEncoder()

lblencoder.fit(lbls)
labels = lblencoder.transform(lbls)

stratified_split = StratifiedKFold(n_splits=5, shuffle=True)

scores = cross_validate(classifier, dvectors, labels, cv=stratified_split, scoring=['f1_micro'])

print("Extend with N-grams:")
print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))

"""## Concatenate BOW and evaluate"""

bow_data = [" ".join([t[0] for t in inst.context]) for inst in senseval.instances('interest.pos')]
bow_lbls = [inst.senses[0] for inst in senseval.instances('interest.pos')]

bow_vectorizer = CountVectorizer()
bow_classifier = MultinomialNB()
bow_lblencoder = LabelEncoder()

stratified_split = StratifiedKFold(n_splits=5, shuffle=True)

bow_vectors = bow_vectorizer.fit_transform(bow_data)

# encoding labels for multi-calss
bow_lblencoder.fit(bow_lbls)
bow_labels = lblencoder.transform(bow_lbls)

print(bow_vectors.shape, type(bow_vectors))
print(dvectors.shape, type(dvectors))

test_features = np.concatenate((bow_vectors.toarray(), dvectors), axis=1)

scores = cross_validate(classifier, test_features, labels, cv=stratified_split, scoring=['f1_micro'])

print("Concatenate BOW:")
print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))

"""## Evaluate Lesk Original"""

def preprocess(text):
    mapping = {"NOUN": wordnet.NOUN, "VERB": wordnet.VERB, "ADJ": wordnet.ADJ, "ADV": wordnet.ADV}
    sw_list = stopwords.words('english')

    lem = WordNetLemmatizer()

    # tokenize, if input is text
    tokens = nltk.word_tokenize(text) if type(text) is str else text
    # pos-tag
    tagged = nltk.pos_tag(tokens, tagset="universal")
    # lowercase
    tagged = [(w.lower(), p) for w, p in tagged]
    # optional: remove all words that are not NOUN, VERB, ADJ, or ADV (i.e. no sense in WordNet)
    tagged = [(w, p) for w, p in tagged if p in mapping]
    # re-map tags to WordNet (return orignal if not in-mapping, if above is not used)
    tagged = [(w, mapping.get(p, p)) for w, p in tagged]
    # remove stopwords
    tagged = [(w, p) for w, p in tagged if w not in sw_list]
    # lemmatize
    tagged = [(w, lem.lemmatize(w, pos=p), p) for w, p in tagged]
    # unique the list
    tagged = list(set(tagged))
    return tagged



def original_lesk(context_sentence, ambiguous_word, pos=None, synsets=None, majority=False, features=None):

    context_senses = get_sense_definitions(set(context_sentence)-set([ambiguous_word]))
    if synsets is None:
        synsets = get_sense_definitions(ambiguous_word)[0][1]

    if pos:
        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]

    if not synsets:
        return None
    scores = []
    # print(synsets)
    for senses in context_senses:
        for sense in senses[1]:
            if features is not None:
                # Aggiungi le feature al contesto per il calcolo del punteggio
                score = get_top_sense(sense[1] + features.tolist(), synsets)
            else:
                score = get_top_sense(sense[1], synsets)
            scores.append(score)

    if len(scores) == 0:
        return synsets[0][0]

    if majority:
        filtered_scores = [x[1] for x in scores if x[0] != 0]
        if len(filtered_scores) > 0:
            best_sense = Counter(filtered_scores).most_common(1)[0][0]
        else:
            # Almost random selection
            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]
    else:
        _, best_sense = max(scores)
    return best_sense


def get_sense_definitions(context):
    # input is text or list of strings
    lemma_tags = preprocess(context)
    # let's get senses for each
    senses = [(w, wordnet.synsets(l, p)) for w, l, p in lemma_tags]

    # let's get their definitions
    definitions = []
    for raw_word, sense_list in senses:
        if len(sense_list) > 0:
            # let's tokenize, lowercase & remove stop words
            def_list = []
            for s in sense_list:
                defn = s.definition()
                # let's use the same preprocessing
                tags = preprocess(defn)
                toks = [l for w, l, p in tags]
                def_list.append((s, toks))
            definitions.append((raw_word, def_list))
    return definitions


def get_top_sense(words, sense_list):
    # get top sense from the list of sense-definition tuples
    # assumes that words and definitions are preprocessed identically
    val, sense = max((len(set(words).intersection(set(defn))), ss) for ss, defn in sense_list)
    return val, sense

mapping = {
    'interest_1': 'interest.n.01',
    'interest_2': 'interest.n.03',
    'interest_3': 'pastime.n.01',
    'interest_4': 'sake.n.01',
    'interest_5': 'interest.n.05',
    'interest_6': 'interest.n.04',
}

refs = {k: set() for k in mapping.values()}
hyps = {k: set() for k in mapping.values()}
refs_list = []
hyps_list = []

synsets = []
for ss in tqdm(wordnet.synsets('interest', pos='n')):
    if ss.name() in mapping.values():
        defn = ss.definition()
        tags = preprocess(defn)
        toks = [l for w, l, p in tags]
        synsets.append((ss,toks))

for i, inst in tqdm(enumerate(senseval.instances('interest.pos'))):
    txt = [t[0] for t in inst.context]
    raw_ref = inst.senses[0] # let's get first sense

    test_instance_features = test_features[i]

    hyp = original_lesk(txt, txt[inst.position], synsets=synsets, majority=True, features=test_instance_features).name()
    ref = mapping.get(raw_ref)

    # for precision, recall, f-measure
    refs[ref].add(i)
    hyps[hyp].add(i)

    # for accuracy
    refs_list.append(ref)
    hyps_list.append(hyp)

print("Lesk original: ")
print("Acc:", round(accuracy(refs_list, hyps_list), 3))

for cls in hyps.keys():
    p = precision(refs[cls], hyps[cls])
    r = recall(refs[cls], hyps[cls])
    f = f_measure(refs[cls], hyps[cls], alpha=1)

    print("{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}".format(cls, p, r, f, len(refs[cls])))

"""## Evaluate Lesk Similarity"""

def lesk_similarity(context_sentence, ambiguous_word, similarity="resnik", pos=None,
                    synsets=None, majority=True):
    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))

    if synsets is None:
        synsets = get_sense_definitions(ambiguous_word)[0][1]

    if pos:
        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]

    if not synsets:
        return None

    scores = []

    # Here you may have some room for improvement
    # For instance instead of using all the definitions from the context
    # you pick the most common one of each word (i.e. the first)
    for senses in context_senses:
        for sense in senses[1]:
            scores.append(get_top_sense_sim(sense[0], synsets, similarity))

    if len(scores) == 0:
        return synsets[0][0]

    if majority:
        filtered_scores = [x[1] for x in scores if x[0] != 0]
        if len(filtered_scores) > 0:
            best_sense = Counter(filtered_scores).most_common(1)[0][0]
        else:
            # Almost random selection
            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]
    else:
        _, best_sense = max(scores)

    return best_sense


#semcor_ic = wordnet_ic.ic('ic-semcor.dat')
def get_top_sense_sim(context_sense, sense_list, similarity):
    # get top sense from the list of sense-definition tuples
    # assumes that words and definitions are preprocessed identically
    scores = []
    for sense in sense_list:
        ss = sense[0]
        if similarity == "path":
            try:
                scores.append((context_sense.path_similarity(ss), ss))
            except:
                scores.append((0, ss))
        elif similarity == "lch":
            try:
                scores.append((context_sense.lch_similarity(ss), ss))
            except:
                scores.append((0, ss))
        elif similarity == "wup":
            try:
                scores.append((context_sense.wup_similarity(ss), ss))
            except:
                scores.append((0, ss))
        elif similarity == "resnik":
            try:
                scores.append((context_sense.res_similarity(ss, semcor_ic), ss))
            except:
                scores.append((0, ss))
        elif similarity == "lin":
            try:
                scores.append((context_sense.lin_similarity(ss, semcor_ic), ss))
            except:
                scores.append((0, ss))
        elif similarity == "jiang":
            try:
                scores.append((context_sense.jcn_similarity(ss, semcor_ic), ss))
            except:
                scores.append((0, ss))
        else:
            print("Similarity metric not found")
            return None
    val, sense = max(scores)
    return val, sense

sim_refs = {k: set() for k in mapping.values()}
sim_hyps = {k: set() for k in mapping.values()}
sim_refs_list = []
sim_hyps_list = []

sim_synsets = []
for ss in wordnet.synsets('interest', pos='n'):
    if ss.name() in mapping.values():
        defn = ss.definition()
        tags = preprocess(defn)
        toks = [l for w, l, p in tags]
        sim_synsets.append((ss,toks))

for i, inst in tqdm(enumerate(senseval.instances('interest.pos'))):
    txt = [t[0] for t in inst.context]
    raw_ref = inst.senses[0] # let's get first sense

    test_instance_features = test_features[i]

    hyp = lesk_similarity(txt, txt[inst.position], synsets=sim_synsets, majority=True).name()
    ref = mapping.get(raw_ref)

    # for precision, recall, f-measure
    sim_refs[ref].add(i)
    sim_hyps[hyp].add(i)

    # for accuracy
    sim_refs_list.append(ref)
    sim_hyps_list.append(hyp)

print("Lesk similarity:")
print("Acc:", round(accuracy(sim_refs_list, sim_hyps_list), 3))

for cls in sim_hyps.keys():
    p = precision(sim_refs[cls], sim_hyps[cls])
    r = recall(sim_refs[cls], sim_hyps[cls])
    f = f_measure(sim_refs[cls], sim_hyps[cls], alpha=1)

    if(p is None):
      p= 0.0
    if(f is None):
      f = 0.0

    print(cls, "\t\t", round(p,3), "\t\t", round(r,3), "\t\t", round(f,3), "\t\t", len(sim_refs[cls]))

    #print("{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}".format(cls, p, r, f, len(sim_refs[cls])))